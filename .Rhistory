library(tidyverse)
library(tidymodels)
library(vroom)
library(dplyr)
library(skimr)
library(GGally)
library(DataExplorer)
library(ggplot2)
library(patchwork)
library(poissonreg)
library(lubridate)
library(rpart)
library(glmnet)
bikeshare <- vroom("train.csv")
biketest <- vroom("test.csv")
## Create a recipe
my_recipe <- recipe(count ~ season + holiday + workingday + weather + temp + atemp + humidity + windspeed + datetime, data = bikeshare) %>%
step_time(datetime, features = "hour") %>%  # Extract hour from datetime
step_date(datetime, features = "dow") %>%   # Extract day of week (dow) from datetime
step_mutate(
datetime_hour = as.factor(datetime_hour),
datetime_dow = as.factor(datetime_dow),
weather = ifelse(weather == 4, 3, weather),
weather = factor(weather, levels = 1:3, labels = c("Clear", "Mist", "Snow/Rain")), # Convert weather to factor
season = factor(season, levels = 1:4, labels = c("Spring", "Summer", "Fall", "Winter")))%>%
#count = log(count))%>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% # Make mean 0, sd=1
step_rm(datetime)
## Penalized regression model
preg_model <- linear_reg(penalty=7, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=bikeshare)
penalized_predictions <- predict(preg_wf, new_data= biketest)
kaggle_penalized_submission <- penalized_predictions %>%
bind_cols(., biketest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count))%>%
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_penalized_submission, file="./PenalizedPreds.csv", delim=",")
library(poissonreg) #if you want to do penalized, poisson regression2
## Penalized regression model
preg_model <- linear_reg(penalty=tune(),
mixture=tune()) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
## Set Workflow9
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = L) ## L^2 total tuning possibilities
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 6) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(trainData, v = 6, repeats=1)
## Split data for CV
folds <- vfold_cv(bikeshare, v = 6, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best("rmse")
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best("rmse")
rlang::last_trace()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "rmse")
## Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=trainData)
## Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bikeshare)
## Predict
final_wf %>%
predict(new_data = biketest)
#Tuning
library(tidymodels)
library(poissonreg) #if you want to do penalized, poisson regression
## Penalized regression model
preg_model <- linear_reg(penalty=tune(),
mixture=tune()) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
## Set Workflow9
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(bikeshare, v = 3, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "rmse")
## Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bikeshare)
## Predict
final_wf %>%
predict(new_data = biketest)
## Set Workflow9
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 6) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(bikeshare, v = 6, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "rmse")
## Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bikeshare)
## Predict
final_wf %>%
predict(new_data = biketest)
## Predict
tune_predict <- final_wf %>%
predict(new_data = biketest)
kaggle_tuning_submission <- tune_predict %>%
bind_cols(., biketest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count))%>%
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
vroom_write(x=kaggle_tuning_submission, file="./TuningPreds.csv", delim=",")
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # What R function to use
set_mode("regression")
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # What R function to use
set_mode("regression")
## Create a workflow with model & recipe
tree_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
grid_of_tuning_params2 <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 6) ## L^2 total tuning possibilities
folds2 <- vfold_cv(bikeshare, v = 6, repeats=1)
CV_results2 <- tree_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params2,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # What R function to use
set_mode("regression")
## Create a workflow with model & recipe
tree_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
grid_of_tuning_params2 <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 6) ## L^2 total tuning possibilities
folds2 <- vfold_cv(bikeshare, v = 6, repeats=1)
CV_results2 <- tree_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params2,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
